%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Projects}

%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------

\cventry
	{Generative Models for Molecule Design}
	{Prof. Piyush Rai}
	{Undergraduate Project}
	{Spring 2019}
	{
		\begin{cvitems}
			\item Modified the Junction Tree VAE model to introduce binary latent variables based on a RBM prior allowing interaction between the graph and the tree latent variables and, therefore, affording a more expressive as well as more interpretable latent model
		\end{cvitems}
	}

%-------------------------------------------------------------------------------
\cventry
	{Discrete VAEs and Stochastic Block Models}
	{Prof. Piyush Rai}
	{Undergraduate Project}
	{Fall 2018}
	{
		\begin{cvitems}
			\item Surveyed continuous relaxations to discrete latent variables such as Gumbel-Softmax, Spike-and-Exp, Overlapping, Gumbolt, etc.
			\item Implemented GumBolt relaxation for RBM prior using tensorflow
			\item Augmented GVAEs with binary latent embeddings to offer interpretable latent representations, imitating mixed membership models
			\item Employed the resultant model for link prediction on graph datasets (Citeseer and Cora) and achieved superior results to baseline models
		\end{cvitems}
	}

%-------------------------------------------------------------------------------
\cventry
	{Mixture of Experts using Discrete VAE}
	{Prof. Arnab Bhattacharya}
	{CS685: Data Mining}
	{Fall 2018}
	{
		\begin{cvitems}
			\item Extended the VaDE model to infer cluster assignments using a deep neural network and employed Virtual Adversarial training
			\item The proposed model worked comparable to VaDE on clustering tasks without the need for careful layer wise pretraining
			\item Extended the proposed model as a gating function for Mixture of Expert tasks and achieved better performance than baseline models
		\end{cvitems}
	}

%-------------------------------------------------------------------------------
\cventry
	{Incremental Neural Networks Training}
	{Prof. Purushottam Kar}
	{CS777: Learning Theory}
	{Spring 2018}
	{
		\begin{cvitems}
			\item Two layer NNs can be represented as an ensemble of multiple single node hidden layer networks, which can be individually trained using generic boosting methods (gradient boosting), which also afford definite theoretical convergence guarantees
			\item Applied gradient boosting to train two layer networks incrementally and studied the convergence analysis under various constraints
			\item Applied incremental training as pre-training, along with backpropagation for fine-tuning and observed remarkably better convergence
		\end{cvitems}
	}

%-------------------------------------------------------------------------------
\cventry
	{Survey: Methods for Convex Optimization}
	{Prof. Purushottam Kar}
	{IITK}
	{Sprint 2018}
	{}
	
%-------------------------------------------------------------------------------
\cventry
	{Machine Comprehension using Match-LSTM}
	{Prof. Harish Karnick}
	{IITK}
	{Sprint 2018}
	{}

%-------------------------------------------------------------------------------
\cventry
	{Java to X86 Assembly Compiler}
	{Prof. Subhajit Roy}
	{IITK}
	{Spring 2018}
	{}

%-------------------------------------------------------------------------------
\cventry
	{NachOS Operating System}
	{Prof. Mainak Chaudhuri}
	{IITK}
	{Fall 2017}
	{}

%-------------------------------------------------------------------------------
\cvcomment{* Code and reports for all projects available at \underline{\href{https://www.github.com/fat-fighter}{github://fat-fighter}}}
