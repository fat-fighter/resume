%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Projects}

%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------

\cventry
	{Bayesian ML for Predicting Underground Water Levels}
    {Dr.\ Alp Kucukelbir (Fero Labs)}
	{COMS 6998: Probabilisitic Programming, CU}
	{Sep 2019 --- Nov 2019}
	{%
		\begin{cvitems}
			\item Modeled underground water levels in a sub-region in Rajasthan (India) based on readings from sparse observatory wells
			\item Used a HMM-modulated kernel regression to model temporal and spatial patterns dependent on satellite observations of farmland
		\end{cvitems}
	}

\cventry
	{Implementing and Analysis of Adaptive Learning of Neural Networks}
	{Dr.\ Satyen Kale (Google Research)}
	{COMS 4995: Optimization Methods, CU}
	{Sep 2019 --- Nov 2019}
	{%
		\begin{cvitems}
			\item Implementated \href{https://arxiv.org/abs/1607.01097}{AdaNet} using PyTorch and analysed its performance with different settings of hyperparameters
			\item Improved hyperparameter sensitivity by adaptively changing the subnetwork width without compromising on performance
		\end{cvitems}
	}

\cventry
	{Discrete VAEs and Stochastic Block Models}
	{Prof.\ Piyush Rai}
	{Undergraduate Project, IITK}
	{Aug 2018 --- Jan 2019}
	{%
		\begin{cvitems}
			\item Surveyed continuous relaxations to discrete latent variables and implemented GumBolt relaxation for RBM prior using tensorflow
			\item Augmented GVAEs with binary latent embeddings to offer interpretable latent representations, imitating mixed membership models
			\item Employed the resultant model for link prediction on graph datasets (Citeseer and Cora) and achieved superior results to baseline models
		\end{cvitems}
	}

%-------------------------------------------------------------------------------
\cventry
	{Incremental Neural Networks Training}
	{Prof.\ Purushottam Kar}
	{CS777: Learning Theory, IITK}
	{Jan 2018 --- Apr 2018}
	{%
		\begin{cvitems}
			\item Employed the concept of training ensembles using gradient boosting for training two layer networks by representing two layer networks as an ensemble of single hidden node networks affording definite theoretical convergence guarantees
			\item Studied the convergence analysis of incremental training under various constraints and assumptions
			\item Applied incremental training as pre-training, along with backpropagation for fine-tuning, and observed remarkably better convergence
		\end{cvitems}
	}
	
%-------------------------------------------------------------------------------
%\cvcomment{* Code and reports for all projects available at \underline{\href{https://www.github.com/fat-fighter}{github://fat-fighter}}}
